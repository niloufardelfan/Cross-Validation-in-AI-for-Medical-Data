{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0002025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Notebook 2: Patient-Wise Validation with Group Cross-Validation\n",
    "#\n",
    "# ## Goals\n",
    "# * Understand the critical concept of **Information Leakage** in medical datasets.\n",
    "# * Learn why standard K-Fold/Stratified K-Fold fail when multiple samples exist per patient.\n",
    "# * Implement **Patient-Wise (Group) Cross-Validation** using `GroupKFold`.\n",
    "# * Understand and implement `LeaveOneGroupOut` (LOGO).\n",
    "# * Demonstrate how `GroupKFold` prevents patient overlap between training and validation folds.\n",
    "#\n",
    "# **This is arguably one of the most important validation concepts for medical AI.**\n",
    "\n",
    "# ## 1. The Problem: Information Leakage from Non-Independent Samples\n",
    "#\n",
    "# In many medical datasets, we have multiple data points (samples) originating from the same individual patient or subject. Examples:\n",
    "# *   Multiple MRI slices from the same patient scan.\n",
    "# *   Several ECG readings over time from one person.\n",
    "# *   Multiple pathology slide images from a single biopsy.\n",
    "# *   Longitudinal data from Electronic Health Records (EHRs) for one patient.\n",
    "#\n",
    "# These samples are **not statistically independent**. Samples from the same patient are likely to be more similar to each other than to samples from different patients due to shared underlying biology, genetics, lifestyle, etc.\n",
    "#\n",
    "# **What happens if we use standard K-Fold or Stratified K-Fold?**\n",
    "# These methods split data at the *sample* level. It's highly likely that for a given fold, some samples from Patient A will end up in the training set, while other samples from the *same* Patient A end up in the validation set.\n",
    "#\n",
    "# **This is Information Leakage:** The model learns patient-specific features during training. When it evaluates samples from the same patient in the validation set, it performs well not necessarily because it learned a generalizable biological pattern, but because it recognizes the patient it saw in training.\n",
    "#\n",
    "# **Result:** The validation score becomes **artificially inflated**, giving a false sense of high performance. The model may fail drastically when deployed on *new, unseen patients*.\n",
    "#\n",
    "# **Goal:** We typically want to estimate how well the model generalizes to **new patients**, not just new samples from patients already seen.\n",
    "\n",
    "# ## 2. The Solution: Group (Patient-Wise) Cross-Validation\n",
    "#\n",
    "# Group CV techniques ensure that all samples belonging to the same group (e.g., patient) are kept together within the *same* split (either all in training or all in validation for a given fold).\n",
    "#\n",
    "# *   **`GroupKFold`**: Partitions *groups* into K folds. Each group is assigned to exactly one validation fold across the K iterations.\n",
    "# *   **`LeaveOneGroupOut` (LOGO)**: Iterates through each unique group, holding out that entire group for validation and training on all other groups. (Equivalent to `GroupKFold` where `n_splits` equals the number of unique groups).\n",
    "\n",
    "# ## 3. Setup and Data\n",
    "#\n",
    "# We need the development data including the crucial `groups_dev` array (patient IDs) generated in Notebook 0.\n",
    "\n",
    "# +\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    KFold, StratifiedKFold, GroupKFold, LeaveOneGroupOut,\n",
    "    cross_val_score, cross_validate # Use cross_validate for more metrics\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import time\n",
    "\n",
    "# %matplotlib inline\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# Assume X_dev, y_dev, groups_dev are loaded from Notebook 0\n",
    "# If you didn't use %store or are running standalone, regenerate or load them here.\n",
    "RANDOM_STATE = 42\n",
    "try:\n",
    "    X_dev.shape # Check if variable exists\n",
    "    print(\"Using data loaded from previous notebook.\")\n",
    "    # Ensure groups are integers (required by scikit-learn GroupKFold)\n",
    "    groups_dev = groups_dev.astype(int)\n",
    "except NameError:\n",
    "    print(\"Generating synthetic data for standalone execution...\")\n",
    "    from sklearn.datasets import make_classification\n",
    "    N_SAMPLES_DEV = 400\n",
    "    N_FEATURES = 20\n",
    "    N_CLASSES = 2\n",
    "    N_PATIENTS_DEV = 80\n",
    "    IMBALANCE = 0.8\n",
    "    X_dev, y_dev = make_classification(\n",
    "        n_samples=N_SAMPLES_DEV, n_features=N_FEATURES, n_informative=10, n_redundant=5, n_repeated=0,\n",
    "        n_classes=N_CLASSES, n_clusters_per_class=2, weights=[IMBALANCE, 1.0 - IMBALANCE],\n",
    "        flip_y=0.05, class_sep=0.8, random_state=RANDOM_STATE\n",
    "    )\n",
    "    samples_per_patient = N_SAMPLES_DEV // N_PATIENTS_DEV\n",
    "    groups_dev = np.repeat(np.arange(N_PATIENTS_DEV), samples_per_patient)\n",
    "    remaining_samples = N_SAMPLES_DEV % N_PATIENTS_DEV\n",
    "    if remaining_samples > 0:\n",
    "        groups_dev = np.concatenate([groups_dev, np.random.choice(N_PATIENTS_DEV, remaining_samples)])\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    np.random.shuffle(groups_dev)\n",
    "    groups_dev = groups_dev.astype(int)\n",
    "    print(f\"Generated X_dev shape: {X_dev.shape}, y_dev shape: {y_dev.shape}, groups_dev shape: {groups_dev.shape}\")\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(solver='liblinear', random_state=RANDOM_STATE)\n",
    "\n",
    "# Calculate number of unique groups (patients)\n",
    "n_unique_groups = len(np.unique(groups_dev))\n",
    "print(f\"\\nNumber of unique patients in Dev set: {n_unique_groups}\")\n",
    "# -\n",
    "\n",
    "# ## 4. Applying GroupKFold\n",
    "#\n",
    "# `GroupKFold` requires the `groups` array to be passed during splitting. `n_splits` cannot be greater than the number of unique groups. It does *not* shuffle by default; the assignment of groups to folds is deterministic based on their first appearance.\n",
    "\n",
    "# +\n",
    "# Ensure n_splits is not greater than the number of unique groups\n",
    "N_SPLITS_GROUP = 5\n",
    "if N_SPLITS_GROUP > n_unique_groups:\n",
    "    N_SPLITS_GROUP = n_unique_groups\n",
    "    print(f\"Warning: n_splits reduced to {N_SPLITS_GROUP} (number of unique groups)\")\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_SPLITS_GROUP)\n",
    "\n",
    "print(f\"\\n--- Running {N_SPLITS_GROUP}-Fold Group Cross-Validation ---\")\n",
    "\n",
    "# Using cross_validate to get multiple scores easily\n",
    "scoring_metrics = ['accuracy', 'roc_auc']\n",
    "\n",
    "start_time = time.time()\n",
    "# **Crucially, pass the 'groups' array to cross_validate**\n",
    "gkf_results = cross_validate(\n",
    "    model,\n",
    "    X_dev,\n",
    "    y_dev,\n",
    "    groups=groups_dev, # THIS IS THE KEY DIFFERENCE\n",
    "    cv=gkf,\n",
    "    scoring=scoring_metrics,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False # Don't need train scores for this demo\n",
    ")\n",
    "gkf_time = time.time() - start_time\n",
    "\n",
    "print(\"\\nGroupKFold Results:\")\n",
    "print(f\"  Fold Test Accuracies: {gkf_results['test_accuracy']}\")\n",
    "print(f\"  Mean Test Accuracy:   {gkf_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"  Std Test Accuracy:    {gkf_results['test_accuracy'].std():.4f}\")\n",
    "print(f\"\\n  Fold Test AUCs:       {gkf_results['test_roc_auc']}\")\n",
    "print(f\"  Mean Test AUC:        {gkf_results['test_roc_auc'].mean():.4f}\")\n",
    "print(f\"  Std Test AUC:         {gkf_results['test_roc_auc'].std():.4f}\")\n",
    "print(f\"\\nTime taken: {gkf_time:.2f} seconds\")\n",
    "\n",
    "# Compare with StratifiedKFold (often overly optimistic if groups exist)\n",
    "print(\"\\n--- For Comparison: Running Stratified K-Fold (potential leakage) ---\")\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS_GROUP, shuffle=True, random_state=RANDOM_STATE)\n",
    "skf_results = cross_validate(\n",
    "    model, X_dev, y_dev, # No groups argument here!\n",
    "    cv=skf, scoring=scoring_metrics, n_jobs=-1\n",
    ")\n",
    "print(\"\\nStratifiedKFold Results (POTENTIALLY INFLATED):\")\n",
    "print(f\"  Mean Test Accuracy:   {skf_results['test_accuracy'].mean():.4f} (+/- {skf_results['test_accuracy'].std():.4f})\")\n",
    "print(f\"  Mean Test AUC:        {skf_results['test_roc_auc'].mean():.4f} (+/- {skf_results['test_roc_auc'].std():.4f})\")\n",
    "# -\n",
    "\n",
    "# **Observation:** Notice if the `GroupKFold` scores (especially AUC) are noticeably lower or have a different standard deviation compared to the `StratifiedKFold` scores. If they are, it strongly suggests that standard CV was suffering from information leakage, and the `GroupKFold` results provide a more realistic estimate of performance on *new patients*.\n",
    "\n",
    "# ## 5. Demonstrating Group Separation Manually\n",
    "#\n",
    "# Let's iterate through the `GroupKFold` splits manually to explicitly show that patient IDs do not overlap between training and validation sets in any fold.\n",
    "\n",
    "# +\n",
    "print(\"\\n--- Manual Group K-Fold Loop: Verifying Patient Separation ---\")\n",
    "fold_counter = 1\n",
    "patient_overlap_found = False\n",
    "\n",
    "for train_index, val_index in gkf.split(X_dev, y_dev, groups=groups_dev):\n",
    "    print(f\"\\nFold {fold_counter}:\")\n",
    "    X_train_fold, X_val_fold = X_dev[train_index], X_dev[val_index]\n",
    "    y_train_fold, y_val_fold = y_dev[train_index], y_dev[val_index]\n",
    "    groups_train_fold, groups_val_fold = groups_dev[train_index], groups_dev[val_index]\n",
    "\n",
    "    print(f\"  Train size: {len(X_train_fold)}, Val size: {len(X_val_fold)}\")\n",
    "\n",
    "    # Get unique patient IDs in this fold's train and validation sets\n",
    "    train_patients = set(groups_train_fold)\n",
    "    val_patients = set(groups_val_fold)\n",
    "    print(f\"  Unique patients in Train set: {len(train_patients)}\")\n",
    "    print(f\"  Unique patients in Val set:   {len(val_patients)}\")\n",
    "\n",
    "    # THE CRITICAL CHECK: Ensure no patient is in both sets\n",
    "    common_patients_in_fold = train_patients.intersection(val_patients)\n",
    "    print(f\"  Patients common to Train and Val in this fold: {len(common_patients_in_fold)}\")\n",
    "\n",
    "    if len(common_patients_in_fold) > 0:\n",
    "        print(f\"  WARNING: Patient overlap detected in Fold {fold_counter}!\")\n",
    "        patient_overlap_found = True\n",
    "        # break # Stop if overlap found, indicates an issue\n",
    "\n",
    "    fold_counter += 1\n",
    "\n",
    "if not patient_overlap_found:\n",
    "    print(\"\\nSuccess: No patient overlap was detected between train/validation sets in any fold using GroupKFold.\")\n",
    "else:\n",
    "    print(\"\\nError: Patient overlap detected. Check implementation or group assignments.\")\n",
    "# -\n",
    "\n",
    "# ## 6. LeaveOneGroupOut (LOGO)\n",
    "#\n",
    "# This validates the model on each patient group individually. It's computationally more expensive if you have many groups (patients).\n",
    "\n",
    "# +\n",
    "logo = LeaveOneGroupOut()\n",
    "n_splits_logo = logo.get_n_splits(X_dev, y_dev, groups=groups_dev)\n",
    "print(f\"\\n--- Running Leave-One-Group-Out Cross-Validation ({n_splits_logo} Splits) ---\")\n",
    "print(f\"(This corresponds to {n_unique_groups} unique patients)\")\n",
    "\n",
    "# This can be slow if n_unique_groups is large\n",
    "if n_splits_logo <= 100: # Demo limit\n",
    "    start_time = time.time()\n",
    "    logo_results = cross_validate(\n",
    "        model,\n",
    "        X_dev,\n",
    "        y_dev,\n",
    "        groups=groups_dev, # Pass groups!\n",
    "        cv=logo,\n",
    "        scoring=scoring_metrics,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    logo_time = time.time() - start_time\n",
    "\n",
    "    print(\"\\nLOGO Results:\")\n",
    "    print(f\"  Mean Test Accuracy:   {logo_results['test_accuracy'].mean():.4f} (+/- {logo_results['test_accuracy'].std():.4f})\")\n",
    "    print(f\"  Mean Test AUC:        {logo_results['test_roc_auc'].mean():.4f} (+/- {logo_results['test_roc_auc'].std():.4f})\")\n",
    "    print(f\"\\nTime taken: {logo_time:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\nSkipping LOGO execution as number of groups ({n_splits_logo}) is large.\")\n",
    "\n",
    "# -\n",
    "\n",
    "# ## 7. Conclusion\n",
    "#\n",
    "# **If your data has multiple samples per patient (or other grouping factor), using `GroupKFold` (or related group methods) is essential for obtaining a realistic and trustworthy estimate of your model's generalization performance to new, unseen subjects.** Standard K-Fold or Stratified K-Fold will likely produce overly optimistic results due to information leakage. Always check your data structure and apply patient-wise validation when appropriate in medical AI tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
